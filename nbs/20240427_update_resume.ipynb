{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b904f61a4a2dbaf",
   "metadata": {},
   "source": "# Updating My Resume"
  },
  {
   "cell_type": "markdown",
   "id": "efaa9fa84ecc5fd1",
   "metadata": {},
   "source": [
    "\n",
    "My resume is a bit outdated, [the last version (atm) was made in 2013](https://github.com/mudiarto/resume/tree/c263795bd5683e640a0a7d2f4572f74256e0ae40). I need to update it, and while I'm learning ML & LLM, I suppose it is a good way to experiment and using it to update my resume.\n",
    "\n",
    "I found couple resources/tools that I want to use for my resume:\n",
    "\n",
    "- JSON schema for resume: [JSON Resume](https://jsonresume.org/)\n",
    "- This article in Medium on [restraining LLM to JSON format](https://mychen76.medium.com/practical-techniques-to-constraint-llm-output-in-json-format-e3e72396c670)\n",
    "\n",
    "So the goal is to use LLM to generate my resume in JSON format based on JSON Resume schema.\n",
    "The input of the LLM will be somewhat unstructured data of projects i've done, the values I've created, and skills I have, experiences, etc. The output is hopefully a good resume that are interesting and relevant to the job I'm applying for (not that I'm applying for any job right now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77ca9cd0308cd8",
   "metadata": {},
   "source": [
    "## 1. Try to restrain LLM to JSON format - trial 1 - Grammar\n",
    "\n",
    "the article I linked above has couple of alternative methods to restrain LLM to JSON format. I don't like the \"Prompt Engineering\" approach, since it seems so unreliable. After reading the article, I'd like to try the \"Grammar\" approach. \n",
    "\n",
    "The scripts on that page requires [llama.cpp](https://github.com/ggerganov/llama.cpp) - which I currently don't have, so I will need to install it first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e981bb00d5ffeb",
   "metadata": {},
   "source": [
    "### Prerequisite - Install llama.cpp\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Clone the repo\n",
    "```bash\n",
    "# go to your project directory in your computer, clone the repo\n",
    "git clone git@github.com:ggerganov/llama.cpp.git\n",
    "```\n",
    "2. Build llama.cpp (using CUDA)\n",
    "```bash\n",
    "# build using make\n",
    "make\n",
    "\n",
    "# I have NVIDIA GPU, so I will build the CUDA version\n",
    "make LLAMA_CUDA=1\n",
    "# i got this error: \n",
    "# bin/sh: 1: nvcc: not found\n",
    "\n",
    "# I need to install nvidia-cuda-toolkit\n",
    "sudo apt install nvidia-cuda-toolkit\n",
    "\n",
    "# try to build again\n",
    "make LLAMA_CUDA=1\n",
    "# I got another error: \n",
    "# Makefile:657: *** I ERROR: For CUDA versions < 11.7 a target CUDA architecture must be explicitly provided \n",
    "# via environment variable CUDA_DOCKER_ARCH, e.g. by running \"export CUDA_DOCKER_ARCH=compute_XX\" on Unix-like \n",
    "# systems, where XX is the minimum compute capability that the code needs to run on. \n",
    "# A list with compute capabilities can be found here: https://developer.nvidia.com/cuda-gpus .  Stop.\n",
    "\n",
    "# I need to set the CUDA_DOCKER_ARCH\n",
    "# see the list here: https://developer.nvidia.com/cuda-gpus\n",
    "export CUDA_DOCKER_ARCH=compute_86\n",
    "\n",
    "# try to build again\n",
    "make LLAMA_CUDA=1\n",
    "\n",
    "# built successfully !\n",
    "```\n",
    "\n",
    "<script src=\"https://asciinema.org/a/za37O363oYVDkbKwLWcGMB0sS.js\" id=\"asciicast-656691\" async=\"true\"></script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2d3143ff5580b",
   "metadata": {},
   "source": [
    "### Prerequisite - Install LLM Models\n",
    "\n",
    "- I need models to run with llama.cpp. I previously used [ollama](https://ollama.org) which makes downloading models very simple. Now I need to understand how to download the models manually.\n",
    "- I'm open to any models, I want to try with either llama3 models which seems very powerful, or phi3 which is more lightweight.\n",
    "- **Q**: it looks like llama.cpp works with gguf format - which looks different from ollama, which is like a blob file. How do I download it?\n",
    "    - **Note**: It  looks like I can download/find those models in [huggingface](https://huggingface.co/models) - but I need to understand how to download and convert it to gguf format.   \n",
    "    - **A**: Found the instruction from PrunaAI (from down below)\n",
    "- **Q**: What model to start with?\n",
    "    - **A**: I want to start with phi3 model, since it seems smaller and hopefully faster to download and run. I want to use 128k because I want to have more context/tokens (i.e. my resume)\n",
    "- **Q**: I want to start with phi3 model, but microsoft doesn't have gguf format for phi-3-mini-128k-instruct: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n",
    "    - **A**: I found it done by PrunaAI: https://huggingface.co/PrunaAI/Phi-3-mini-128k-instruct-GGUF-Imatrix-smashed\n",
    "    - **NOTE**: Shoutout to [PrunaAI](https://www.pruna.ai/) - and https://huggingface.co/PrunaAI  - they make the model smaller and give very good instruction to noobs like me! very thankful for that!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bcae32ccb0104",
   "metadata": {},
   "source": [
    "#### Download the model from hugging face - instructions credit to PrunaAI\n",
    "\n",
    "[Reference Instructions](/how-to-download-and-run-models-from-huggingface.html)\n",
    "\n",
    "Steps/Rationale:\n",
    "\n",
    "* I'm using command line hugging face cli to download the model. I prefer cli so it can be automated later\n",
    "* See my notes below for the steps !!\n",
    "\n",
    "\n",
    "```bash\n",
    "# ensure that huggingface-cli is already installed\n",
    "# if not, instruction is here:  https://huggingface.co/docs/huggingface_hub/en/guides/cli\n",
    "# install hf_transfer: `pip3 install hf_transfer` to speed up download on fast internet connection\n",
    "\n",
    "# ensure you're logged in\n",
    "huggingface-cli login\n",
    "# insert token from hugging face, from: https://huggingface.co/settings/token\n",
    "\n",
    "# validate you're logged in\n",
    "huggingface-cli whoami\n",
    "\n",
    "# change to model directory first\n",
    "cd models/\n",
    "\n",
    "# download the model\n",
    "huggingface-cli download PrunaAI/Phi-3-mini-128k-instruct-GGUF-smashed Phi-3-mini-128k-instruct.IQ3_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# Unfortunately, I got this error:\n",
    "# Repository Not Found for url: https://huggingface.co/PrunaAI/Phi-3-mini-128k-instruct-GGUF-smashed/resolve/main/Phi-3-mini-128k-instruct.IQ3_M.gguf.\n",
    "\n",
    "# I need to find the correct model name\n",
    "huggingface-cli download PrunaAI/Phi-3-mini-128k-instruct-GGUF-Imatrix-smashed/blob/main/Phi-3-mini-128k-instruct.Q5_K_M.gguf  --local-dir . --local-dir-use-symlinks False\n",
    "# I got this error:\n",
    "# huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'PrunaAI/Phi-3-mini-128k-instruct-GGUF-Imatrix-smashed/blob/main/Phi-3-mini-128k-instruct.Q5_K_M.gguf'. Use `repo_type` argument if needed.\n",
    "\n",
    "# third try\n",
    "huggingface-cli download PrunaAI/Phi-3-mini-128k-instruct-GGUF-Imatrix-smashed/resolve/main/Phi-3-mini-128k-instruct.Q5_K_M.gguf  --local-dir . --local-dir-use-symlinks False\n",
    "# I got same error\n",
    "\n",
    "# and now I just realized that I used it incorrectly. The right way is probably this:\n",
    "huggingface-cli download PrunaAI/Phi-3-mini-128k-instruct-GGUF-Imatrix-smashed Phi-3-mini-128k-instruct.IQ3_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# and it works !!!!\n",
    "\n",
    "# However, I just realized that you can just download it manually from hugging face website :( ... \n",
    "# in any case, it is good to know that you can download it using cli for automation purposes\n",
    "\n",
    "```\n",
    "\n",
    "- **Note**: I just realized that I can download the model manually from hugging face website. This is probably a better way if you only need to download it once. But if you need to automate it, you can use the cli.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785287a1cd204ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prerequisite - experiment with llama.cpp\n",
    "\n",
    "```bash\n",
    "# I have the model now, let's experiment with llama.cpp\n",
    "\n",
    "# show helps\n",
    "./main -h | less\n",
    "# helps show interactive mode, looks interesting\n",
    "\n",
    "# first I tried with interactive mode\n",
    "./main -i\n",
    "# apparently I need to specify the model\n",
    "\n",
    "\n",
    "# I tried to use interactive mode with models, but it gave a long weird output\n",
    "./main -m models/Phi-3-mini-128k-instruct.IQ3_M.gguf -i   \n",
    "# long weird random output\n",
    "\n",
    "# I'm trying now with the example from the page I linked above\n",
    "./main -ngl 35 -m models/Phi-3-mini-128k-instruct.IQ3_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<s>[INST] {prompt\\} [/INST]\"\n",
    "# looks promising - it give some results\n",
    "\n",
    "# it looks like llama.cpp doesn't use CUDA - let me see if I can use it\n",
    "# according to llama.cpp, I can use CUDA by setting the environment variable CUDA_VISIBLE_DEVICES\n",
    "# and based on this: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars - I can get the value from nvidia-smi\n",
    "# we need to use nvidia-smi -L to list all gpus\n",
    "nvidia-smi -L\n",
    "\n",
    "# output is something like this:\n",
    "nvidia-smi -L\n",
    "# GPU 0: NVIDIA GeForce RTX 3090 (UUID: GPU-1f000d41-bbba-2144-c214-e4a4fac78d5b)\n",
    "# export CUDA_VISIBLE_DEVICES=GPU-1f000d41-bbba-2144-c214-e4a4fac78d5b\n",
    "\n",
    "\n",
    "# now try it again with instruction mode\n",
    "./main -m models/Phi-3-mini-128k-instruct.IQ3_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 --instruct \"Who is the president of the US\"\n",
    "# didn't work, just showing me the help file\n",
    "\n",
    "# try again without prompt\n",
    "export CUDA_VISIBLE_DEVICES=GPU-1f000d41-bbba-2144-c214-e4a4fac78d5b\n",
    "./main -m models/Phi-3-mini-128k-instruct.IQ3_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 --instruct\n",
    "\n",
    "\n",
    "# apparently to use cuda, i'll need -ngl <some number> where llama will try to offload as many as possible\n",
    "\n",
    "./main -ngl 10000 -m models/Phi-3-mini-128k-instruct.IQ3_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 --instruct\n",
    "# now I see a different warning - maybe I didn't make llama.cpp cleanly before\n",
    "# warning: not compiled with GPU offload support\n",
    "# i'm doing `make clean` & `make LLAMA_CUDA=1` again\n",
    "\n",
    "# I'm trying again with new llama.cpp\n",
    "./main -ngl 10000 -m models/Phi-3-mini-128k-instruct.IQ3_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 --instruct\n",
    "# it works !!\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561a09631431101",
   "metadata": {},
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
